# -*- coding: utf-8 -*-
"""beginner_multimodal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/autogluon/neurips2022-autogluon-workshop/blob/main/notebooks/multimodal/beginner_multimodal.ipynb

# AutoMM for Multimodal Classification with Image + Text + Tabular - Quick Start
:label:`sec_automm_multimodal_beginner`

[![Open In Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/neurips2022-autogluon-workshop/blob/main/notebooks/multimodal/beginner_multimodal.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/neurips2022-autogluon-workshop/blob/main/notebooks/multimodal/beginner_multimodal.ipynb)

AutoGluon Multimodal (a.k.a AutoMM) is a deep learning "model zoo" of model zoos. It can automatically build deep learning models that are suitable for multimodal datasets. You will only need to convert the data into the multimodal dataframe format
and AutoMM can predict the values of one column conditioned on the features from the other columns including images, text, and tabular data.

To start, let's install autogluon and import MultiModalPredictor. Note that you will **have to restart the runtime** if you are using Google Colab / SageMaker Studio Lab.
"""

# !pip install -U -q autogluon
# !mim install -q mmcv-full

import os
import numpy as np
import warnings
from autogluon.core.utils.loaders import load_zip
import pandas as pd
from autogluon.multimodal import MultiModalPredictor
from autogluon_custom_metric_serializable import probabilistic_f1_scorer
import uuid
import multiprocessing as mp
import pylibjpeg
import pydicom
import cv2
import glob
from tqdm.notebook import tqdm
from joblib import Parallel, delayed
from pathlib import Path
import shutil
import sys
import argparse
import torch
import datetime
from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
import yaml


torch.set_float32_matmul_precision('high')

#Create a class for directory that has getter methods for its 2 subdirectories and 2 files called train_images, test_images, train.csv, test.csv
class DatasetDir:
    def __init__(self, path):
        self._path = path
        self._train_images = os.path.join(path, 'train_images')
        self._test_images = os.path.join(path, 'test_images')
        self._train_csv = os.path.join(path, 'train.csv')
        self._test_csv = os.path.join(path, 'test.csv')

    # Define getter methods for the 2 subdirectories and 2 files with attribute names
    @property
    def get_path(self):
        return self._path

    @property
    def get_train_images(self):
        return self._train_images

    @property
    def get_test_images(self):
        return self._test_images

    @property
    def get_train_csv(self):
        return self._train_csv

    @property
    def get_test_csv(self):
        return self._test_csv

# threadsafe function to create a directory
def create_folder(lock, foldername, nocheck=False):
    with lock:
        if os.path.exists(foldername) and not nocheck:
            shutil.rmtree(foldername)  # delete the entire directory tree

        if not os.path.exists(foldername):
            os.makedirs(foldername)
    return

"""
define a function that takes a dataframe and add another column with 
concatinating a specific string to two columns and an extension
"""
def add_column(df, src_col1_name, src_col2_name, new_col_name, str_to_add, extension=None):
    df[new_col_name] = str_to_add + df[src_col1_name].astype(str) + "/" + df[src_col2_name].astype(str)
    if extension is not None:
        df[new_col_name] = df[new_col_name] + extension
    return df

def find_effective_region(image):
    # Load the grayscale mammogram image
    # image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Identify the region of interest (ROI) containing the breast tissue
    # For example, using thresholding:
    image = cv2.convertScaleAbs(image, alpha=(65535.0 / np.max(image)))
    ret, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    breast_contour = max(contours, key=cv2.contourArea)

    # Crop the image to include only the breast tissue
    x, y, w, h = cv2.boundingRect(breast_contour)
    cropped_image = image[y:y+h, x:x+w]
    return cropped_image

def find_largest_connected_component_bbox(image):
    threshold=128
    min_area=1000

    # Load the grayscale image
    # image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

    # Convert the image to 8-bit unsigned integers
    img_8bit = np.uint8(image * 255)

    # Threshold the image
    _, binary = cv2.threshold(img_8bit, threshold, 255, cv2.THRESH_BINARY_INV)

    # Find connected components
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=8)

    # Find the largest connected component, excluding the background (label 0)
    max_area = 0
    max_label = 0
    for i in range(1, num_labels):
        if stats[i, cv2.CC_STAT_AREA] > max_area:
            max_area = stats[i, cv2.CC_STAT_AREA]
            max_label = i

    # Check if a connected component was found
    if max_label == 0:
        raise ValueError("No connected components found")

    # Get the bounding box of the largest connected component
    x = stats[max_label, cv2.CC_STAT_LEFT]
    y = stats[max_label, cv2.CC_STAT_TOP]
    w = stats[max_label, cv2.CC_STAT_WIDTH]
    h = stats[max_label, cv2.CC_STAT_HEIGHT]

    cropped_image = img_8bit[y:y+h, x:x+w]
    return cropped_image

def process(lock, f, png_size=512, save_folder="", extension=".png", croptype=0):
    patient = parent_dir = f.split('/')[-2]
    image_num = f.split('/')[-1][:-4]

    # rewrite code so that if croptype is defined to be 3 or 4, then have extra indirection in the write_dir,
    # so that the cropped images are saved in a separate folder
    # another aspect is that if lock is not none, use lock fo rcreate folder. if lock is none, then dont use lock
    extra_indirection = "cropped_4/" if croptype == 4 else "cropped_3/" if croptype == 3 else ""
    if lock is not None:
        write_dir = save_folder + extra_indirection + f"{parent_dir}/"
        create_folder(lock, write_dir, True)
    else:
        write_dir = save_folder

    dicom = pydicom.dcmread(f)
    img = dicom.pixel_array

    img = (img - img.min()) / (img.max() - img.min())

    if dicom.PhotometricInterpretation == "MONOCHROME1":
        img = 1 - img

    if croptype == 3:
        img = find_effective_region(img)
    elif croptype == 4:
        img = find_largest_connected_component_bbox(img)

    img = cv2.resize(img, (png_size, png_size))

    cv2.imwrite(write_dir + f"{image_num}{extension}", (img * 255).astype(np.uint8))

def create_pngs_from_dicoms(uselock, dataset_folder, png_size=512, extension=".png", croptype=0):
    if uselock:
        manager = mp.Manager()
        lock = manager.Lock()
    else:
        lock = None

    png_size_save_folder = dataset_folder.get_path + f"train_pngs/output" + f"/{png_size}/"
    if uselock:
        create_folder(lock, png_size_save_folder)
    else:
        if not os.path.exists(png_size_save_folder):
            os.makedirs(png_size_save_folder)

    files = glob.glob(dataset_folder.get_train_images + "/*/*.dcm")
    print("Number of files: ", len(files))

    # for uid in tqdm(files[:len(files)]):
    #     process(lock, uid, png_size, png_size_save_folder, extension, croptype)

    _ = Parallel(n_jobs=mp.cpu_count())(
        delayed(process)(lock, uid, png_size, png_size_save_folder, extension, croptype)
        for uid in tqdm(files[:len(files)])
        # for uid in tqdm(files[:200]) # for debug
    )
    return

def runSessions(predict=False, train=False, createpng=False, uselock=False,
                create_test_from_train=False, trainpercent_totest = 0.1,
                dataset_folder='./', png_size=512,
                extension=".png", debug_small_train=False, use_model_to_predict=None,
                use_train_to_test=False, croptype=0):
    warnings.filterwarnings('ignore')

    # If dicom files need to be converted to pngs, call his function
    if createpng:
        create_pngs_from_dicoms(uselock, dataset_folder, png_size, extension, croptype)

    np.random.seed(123)

    """load the CSV files."""
    train_data = pd.read_csv(dataset_folder.get_train_csv, index_col=0)
    if debug_small_train:
        train_data=train_data.iloc[:1000,:] # for debug
    print(train_data.head())

    # Add path for pngs in new DF column
    subfolder = f'{png_size}_cropped_exp_{croptype}/' if croptype == 3 else f'{png_size}/'
    saved_folder = dataset_folder.get_path + f"train_pngs/output/" + subfolder # f"/{png_size}/"
    train_data = add_column(train_data, 'patient_id', 'image_id', 'image_path',
                            saved_folder, extension)
                            # download_dir + 'rsna-breast-cancer', "-1024-pngs/", '.png')
    pd.options.display.max_colwidth = 100
    print(train_data[["patient_id", "image_id", "image_path"]].head())
    train_data.drop(["biopsy", "invasive", "BIRADS", "density", "difficult_negative_case"], axis=1, inplace=True)

    if create_test_from_train:
        np.random.seed(123)
        # Split the train data into a training set and a test set
        train_data, test_data = train_test_split(train_data, test_size=trainpercent_totest, random_state=42)
    elif use_train_to_test:
        test_data = train_data.copy()
    else:
        test_data = pd.read_csv(dataset_folder.get_test_csv, index_col=0)
        print(test_data.head())
        test_data.rename(columns={'prediction_id': 'cancer'}, inplace=True)
        # Add path for pngs in new DF column
        test_data = add_column(test_data, 'patient_id', 'image_id', 'image_path',
                            saved_folder, extension)

    print(train_data.head())
    label_col = 'cancer'

    image_col = 'image_path'
    print("train_data[image_col].iloc[0]:")
    print(train_data[image_col].iloc[0])

    example_row = train_data.iloc[0]
    print("train_data example_row 0:")
    print(example_row)

    """## TRAINING
    Now let's fit the predictor with the training data. Here we set a tight time budget for a quick demo.
    """
    predictor = MultiModalPredictor(label=label_col)
    if train:
        predictor_train_data = train_data.drop(columns=['patient_id', 'image_id'])
        predictor.fit(
            presets="best_quality",
            train_data=predictor_train_data,
            hyperparameters={"env.num_gpus": 1,
                             "optimization.max_epochs": 40,
                             "optimization.patience": 10,
                            #  "model.timm_image.num_epochs": 40,
                            #  "model.timm_image.patience": 10,
                             "env.per_gpu_batch_size": 64 # for 256x256 images
                             # "env.per_gpu_batch_size": 8 # for 1024x1024 images,
                            },
        )

    """## PREDICTION
    Given a multimodal dataframe without the label column, we can predict the labels.
    """
    if predict:
        # test_data_save = pd.DataFrame(data=test_data["cancer"])
        test_data_del_later = pd.DataFrame(data=test_data[["patient_id", "laterality", "cancer"]])
        test_data_save = pd.DataFrame(data=test_data[["patient_id", "laterality", "cancer"]])
        duplicates = test_data_save[test_data_save.index.duplicated(keep=False)]

        # create new column with patient_id and laterality concatenated
        test_data_save["prediction_id"] = test_data_save.apply(lambda x: f"{x['patient_id']}_{x['laterality']}", axis=1)

        # change value of the dataframe column (forget any prediction values)
        test_data['cancer'] = 0
        # change datatype of dataframe column
        test_data['cancer'] = test_data['cancer'].astype('int')

        """ Reorder dataframe columns to match order of another dataframe """
        test_data = test_data[train_data.columns]

        example_row = test_data.iloc[0]
        print("test_data example_row 0:")
        print(example_row)

        # if use_model_to_predict is None:
        #     print("No model specified. Please specify a model to use for prediction")
        #     return
        # elif create_test_from_train:
        #     print("Model path is", predictor.path)
        #     predictor = MultiModalPredictor.load(predictor.path)
        # elif os.path.exists(use_model_to_predict):
        #     predictor = MultiModalPredictor.load(use_model_to_predict)
        # else:
        #     print("Model not found. Please specify a valid model to use for prediction")

        # Note that use_model_to_predict and train are mutually exclusive
        # if use_model_to_predict is given, check its path exists and set predictor to that
        # else if train is true, set predictor to predictor.path
        # else if none of the above, print error and return

        if use_model_to_predict is not None:
            if os.path.exists(use_model_to_predict):
                predictor = MultiModalPredictor.load(use_model_to_predict)
            else:
                print("Model not found. Please specify a valid model to use for prediction")
                return
        elif train:   # Not needed as predictor is already set to predictor.path
            print("Model path is", predictor.path)
        else:
            print("No model specified. Please specify a model to use for prediction")
            return
    else:
        return

    # get unique id for filename based on time
    filenameuid =  datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # predictions = predictor.predict(test_data.drop(columns=label_col))
    predictions_test_data = test_data.drop(columns=['patient_id', 'image_id', label_col])
    predictions = predictor.predict(predictions_test_data)
    print(predictions[:5])
    accuracy_sc = accuracy_score(test_data_save['cancer'], predictions)
    print("prediction accuracy_score:", accuracy_sc)
    prediction_id = test_data['patient_id'].astype(str) + "_" + test_data['laterality']
    predictions = pd.DataFrame({
        'prediction_id': prediction_id,
        'patient_id': test_data['patient_id'],
        'laterality': test_data['laterality'],
        'cancer': test_data_save['cancer'],
        'predictions': predictions
    })
    predictions.set_index('prediction_id', inplace=True)
    # predictions.to_csv("predictions.csv")
    # Save predictions to csv file of name predictions_<filenameuid>.csv
    predictions_file = "output/runpredictions/predictions_" + filenameuid + ".csv"
    print("predictions_file:", predictions_file)
    predictions.to_csv(predictions_file)

    """For classification tasks, we can get the probabilities of all classes."""

    # probas = predictor.predict_proba(test_data.drop(columns=label_col))
    # probas = predictor.predict_proba(test_data.drop(columns=['patient_id', 'image_id', label_col]))
    probas = predictor.predict_proba(predictions_test_data)
    probas["prediction_id"] = test_data['patient_id'].astype(str) + "_" + test_data['laterality']
    probas['patient_id'] = test_data['patient_id']
    probas['laterality'] = test_data['laterality']
    probas['cancer'] = test_data_save['cancer']
    probas = probas.set_index('prediction_id')
    # probas.to_csv("probas.csv")
    probas_file = "output/runpredictions/probas_" + filenameuid + ".csv"
    print("probas_file:", probas_file)
    probas.to_csv(probas_file)
    # probas = probas.iloc[:, 1:2]
    probas_f1 = probas.iloc[:, 1:2]
    probas_f1 = probas_f1.rename(columns={1: 'cancer'})
    print(probas_f1.head())
    print("probabilistic_f1_scorer:", probabilistic_f1_scorer(test_data_save[label_col], probas_f1['cancer']))

    # Rename the 'cancer' column in the predictions DataFrame
    predictions = predictions.rename(columns={'cancer': 'cancer_pred'})

    # Rename the 'cancer' column in the probas DataFrame
    probas = probas.rename(columns={'cancer': 'cancer_prob'})

    # Concatenate the modified predictions and probas DataFrames
    # predictions_probas = pd.concat([predictions, probas], axis=1)

    # Combine predictions and probas into one dataframe and renaming duplicate columns
    predictions_probas = pd.concat([predictions, probas], axis=1)
    predictions_probas = predictions_probas.rename(columns={'cancer_x': 'cancer', 'cancer_y': 'cancer_proba'})
    # predictions_probas = pd.concat([predictions, probas], axis=1)
    # drop columns 'laterality' and 'patient_id' from predictions_probas
    predictions_probas = predictions_probas.drop(columns=['laterality', 'patient_id'])
    predictions_probas_file = "output/runpredictions/predictions_probas_" + filenameuid + ".csv"
    print("predictions_probas_file:", predictions_probas_file)
    # Write predictions_probas to csv file
    predictions_probas.to_csv(predictions_probas_file)

    # rename the last column to "cancer_prob"
    # predictions_probas = predictions_probas.rename(columns={"cancer": "cancer_pred"})
    # predictions_probas = predictions_probas.rename(columns={"cancer.1": "cancer_prob"})

    # add new columns 
    predictions_probas['mean_cancer_prob'] = predictions_probas.groupby('prediction_id')['cancer_prob'].transform('mean')
    predictions_probas['count'] = predictions_probas.groupby('prediction_id')['cancer_prob'].transform('count')
    predictions_probas['max_cancer_prob'] = predictions_probas.groupby('prediction_id')['cancer_prob'].transform('max')

    # get prediction_ids where even one row has the 2nd and 3rd columns with different values
    pred_ids = predictions_probas[predictions_probas.iloc[:, 1] != predictions_probas.iloc[:, 2]]["prediction_id"].unique()

    # create temporary dataframe for rows with matching pred_ids
    temp_df = predictions_probas[predictions_probas['prediction_id'].isin(pred_ids)]

    # print the temporary dataframe
    temp_df_file = "output/runpredictions/temp_df_" + filenameuid + ".csv"
    print("temp_df_file:", temp_df_file)
    temp_df.to_csv(temp_df_file)
    # print(temp_df)

    # update cancer_prob to max_cancer_prob if max_cancer_prob is greater than 0.6
    predictions_probas.loc[['max_cancer_prob'] > 0.9, 'cancer_prob'] = predictions_probas['max_cancer_prob']

    # recalculate mean_cancer_prob after updating cancer_prob
    predictions_probas['mean_cancer_prob'] = predictions_probas.groupby('prediction_id')['cancer_prob'].transform('mean')

    #print updated predictions_probas
    predict_probas_file_updated = "output/runpredictions/predictions_probas_updated_" + filenameuid + ".csv"
    print("predict_probas_file_updated:", predict_probas_file_updated)
    predictions_probas.to_csv(predict_probas_file_updated)
    # print(predictions_probas)

    # drop all columns except prediction_id and mean_cancer_prob
    sub_df = predictions_probas[['prediction_id', 'mean_cancer_prob']].copy()

    # rename mean_cancer_prob to cancer
    sub_df = sub_df.rename(columns={'mean_cancer_prob': 'cancer'})

    # drop duplicate rows
    sub_df = sub_df.drop_duplicates()

    # check for duplicates in prediction_id column
    if sub_df['prediction_id'].duplicated().any():
        print("Warning: There are duplicate prediction_id values in sub_df!")

    # print sub_df to csv file
    sub_df_filename = "output/runpredictions/submission_" + filenameuid + ".csv"
    print("sub_df_filename:", sub_df_filename)
    sub_df.to_csv(sub_df_filename)
    # sub_df.to_csv('submission.csv', index=False)

    # calculate and print new probabilistic_f1_score for sub_df against test_data_save
    # drop duplicate ruows on prediction_id
    test_data_save.drop_duplicates(subset=["prediction_id"], inplace=True)

    # set prediction_id as index
    test_data_save.set_index("prediction_id", inplace=True)

    # check for duplicates based on patient_id and laterality
    duplicates = test_data_save[test_data_save.index.duplicated(keep=False)]

    if len(duplicates) > 0:
        print("Duplicate values found for the same patient and laterality:")
        print(duplicates)
    else:
        print("No duplicate values found.")

    sub_df = sub_df.rename(columns={"cancer": "cancer_pred"})
    test_data_save = test_data_save.rename(columns={"cancer": "cancer_diagnosis"})
    result_df = sub_df.join(test_data_save, on="prediction_id")

    print("probabilistic_f1_scorer:", probabilistic_f1_scorer(result_df['cancer_diagnosis'], result_df['cancer_pred']))

    metrics = [accuracy_score, f1_score, recall_score, precision_score, roc_auc_score]
    y_pred = predictor.predict(test_data.drop(columns=label_col))
    y_true = test_data_del_later[label_col]

    if len(set(y_true)) > 1:  # check if more than one class is present in y_true
        metrics.remove(roc_auc_score)  # remove ROC AUC metric if only one class is present

    scores = {}
    for metric in metrics:
        name = metric.__name__
        score = metric(y_true, y_pred)
        scores[name] = score
        print(f"{name}: {score}")


if __name__ == '__main__':
    mp.freeze_support()
    # Test creation of png images
    # create a lock to synchronize access to directory creation
    dataset_dirname = "./input/rsna-breast-cancer-detection/"
    # dicom_folder = "./input/rsna-breast-cancer-detection/train_images/"
    # save_folder = "./input/rsna-breast-cancer-detection/train_pngs/output/"

    # ## DEFAULT START ##
    png_size = 256
    extension = ".png"
    predict = False
    train = False
    create_test_from_train = False
    createpng = False
    uselock = False
    debug_small_train = False
    # use_model_to_predict = "./AutogluonModels/ag-20230322_053322"
    use_model_to_predict = None
    use_train_to_test = False
    croptype = 0
    # Add a new parameter to the parser called trainpercent_totest
    # This parameter will be used to specify the percentage of the training data to be used for testing
    # The default value is 0.1
    trainpercent_totest = 0.1
    # Add a new parameter to read config.yaml file by default, unless the yaml file path is defined
    yaml_file = None


    # ## DEFAULT END ##

    # ## DEBUG START ##
    # png_size = 512
    # extension = ".png"
    # predict = True
    # train = True
    # create_test_from_train = False
    # createpng = False
    # uselock = False
    # debug_small_train = True
    # use_model_to_predict = None
    # use_train_to_test = True
    # croptype = 3
    # ## DEBUG END ##

    # Create an argument parser object
    parser = argparse.ArgumentParser(description="program for rsna mammography screening kaggle competition")

    # check and disallow use of --use_model_to_predict and --train together

    # Add arguments to the parser
    # parser.add_argument("--dataset_dirname", type=str, default="./input/rsna-breast-cancer-detection/", help="Path to the dataset folder")
    # add_argument for optional parameter yaml_file code below
    parser.add_argument("--yaml_file", type=str, default=yaml_file, help="Path to the optional config yaml file")
    parser.add_argument("--dataset_dirname", type=str, default=dataset_dirname, help="Path to the dataset folder")
    parser.add_argument("--png_size", type=int, default=png_size, help="Size of PNG images")
    parser.add_argument("--extension", type=str, default=extension, help="Extension of image files")
    parser.add_argument("--predict", action="store_true", default=predict, help="Flag to run prediction")
    parser.add_argument("--train", action="store_true", default=train, help="Flag to run training")
    parser.add_argument("--create_test_from_train", action="store_true", default=create_test_from_train, help="Flag to indicate whether to use test.csv or draw from train")
    parser.add_argument("--createpng", action="store_true", default=createpng, help="Flag to create PNG images")
    parser.add_argument("--uselock", action="store_true", default=uselock, help="Flag to use lock")
    parser.add_argument("--debug_small_train", action="store_true", default=debug_small_train, help="Debug by using a small train set")
    parser.add_argument("--use_model_to_predict", type=str, default=use_model_to_predict, help="Path to the model to use for prediction")
    parser.add_argument("--use_train_to_test", action="store_true", default=use_train_to_test, help="Flag to use train.csv as test.csv")
    parser.add_argument("--croptype", type=int, default=croptype, help="Crop type to use, choose from 3 or 4")
    parser.add_argument("--trainpercent_totest", type=float, default=trainpercent_totest, help="Percentage of training data to use for testing")

    # Parse the arguments
    args = parser.parse_args()

    if args.use_model_to_predict is not None and args.train:
        print("Error: --use_model_to_predict and --train cannot be used together")
        exit(1)

    # # check and disallow use of --use_model_to_predict and --train together
    # if args.use_model_to_predict != "" and args.train:
    #     print("Error: --use_model_to_predict and --train cannot be used together")
    #     exit(1)

    # # if use_train_to_test is True, then set train to False
    # if args.use_train_to_test:
    #     args.train = False
    #     print("--use_train_to_test specified, setting --train to False")

    # if train is True, then set use_model_to_predict to ""
    if args.train:
        args.use_model_to_predict = None
        print("--train specified, setting --use_model_to_predict to ''")

    # if use_model_to_predict is not empty, then set train to False
    if args.use_model_to_predict is not None:
        args.train = False
        print("--use_model_to_predict specified, setting --train to False")


    # Access the variables using dot notation
    dataset_folder = DatasetDir(args.dataset_dirname)
    png_size = args.png_size
    extension = args.extension
    predict = args.predict
    train = args.train
    create_test_from_train = args.create_test_from_train
    createpng = args.createpng
    uselock = args.uselock
    debug_small_train = args.debug_small_train
    use_model_to_predict = args.use_model_to_predict
    use_train_to_test = args.use_train_to_test
    croptype = args.croptype
    trainpercent_totest = args.trainpercent_totest
    yaml_file = args.yaml_file

    # Add code to read config file yaml for arguments if it exists
    # Code below
    if os.path.exists(yaml_file):
        with open(yaml_file, 'r') as stream:
            try:
                config = yaml.safe_load(stream)
                # print(config)
                # print(config['png_size'])
                png_size = int(config.get('png_size', png_size))
                extension = config.get('extension', extension)
                predict = bool(config.get('predict', predict))
                train = bool(config.get('train', train))
                create_test_from_train = bool(config.get('create_test_from_train', create_test_from_train))
                createpng = bool(config.get('createpng', createpng))
                uselock = bool(config.get('uselock', uselock))
                debug_small_train = bool(config.get('debug_small_train', debug_small_train))
                use_model_to_predict = config.get('use_model_to_predict', use_model_to_predict)
                use_train_to_test = bool(config.get('use_train_to_test', use_train_to_test))
                croptype = int(config.get('croptype', croptype))
                trainpercent_totest = float(config.get('trainpercent_totest', trainpercent_totest))
            except yaml.YAMLError as exc:
                print(exc)

    # check and disallow use of use_model_to_predict and train together
    # code below
    if use_model_to_predict is not None and train:
        print("Error: use_model_to_predict and train settings cannot be used together")
        exit(1)

    # if train is True, then set use_model_to_predict to ""
    if train:
        use_model_to_predict = None
        print("train specified, setting use_model_to_predict to ''")

    # if use_model_to_predict is not empty, then set train to False
    if use_model_to_predict is not None:
        train = False
        print("use_model_to_predict specified, setting train to False")

    # Print the variables for testing
    print("Arguments:")
    print("yaml_file=", yaml_file)
    print(f"dataset_folder={dataset_folder.get_path}")
    print(f"png_size={png_size}")
    print(f"extension={extension}")
    print(f"predict={predict}")
    print(f"train={train}")
    print(f"create_test_from_train={create_test_from_train}")
    print(f"createpng={createpng}")
    print(f"debug_small_train={debug_small_train}")
    print(f"uselock={uselock}")
    print("use_model_to_predict=", use_model_to_predict)
    print("use_train_to_test=", use_train_to_test)
    print("croptype=", croptype)
    print("trainpercent_totest=", trainpercent_totest) 

    runSessions(predict, train, createpng, uselock, create_test_from_train, trainpercent_totest,
                dataset_folder, png_size, extension, debug_small_train,
                use_model_to_predict, use_train_to_test, croptype)
